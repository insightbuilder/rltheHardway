13/02
- Diving into RL Basics:
    > RL Official definition: Solving control / decision making tasks by building agents that interact and learnt from environment by getting rewards
    > RL FrameWork: State, Action, Reward, Next State. (even not being dead is a reward!!!)
    > RL goal is to maximize the "Expected Return", or in other words Cumulative Reward
    > Markov Property: States that Agent needs only the current state to take act/ take decision, not the entire history of states
    > Observed State is partial description of the environment(2dScroller), State is "complete" description of the environment(game board). 
    > Action Space: Set of possible action that can be taken by Agent. Continuous(selfDriving Car) or Discrete(Mario game, 4 directions)
    > Reward and discounting is the only feedback to the agent in RL. More predictable near term rewards, Vs unpredictable longterm reward. 
        <> Discounting happens depending on whether we are likely to get the reward, (as it could be dangerous or unavailable)
        <> Or the reward will be available in the future
        🎇 Measured using Gamma: Between 0 and 1 usually 0.95 to 0.99.  
        🥌 leads to discounted expected cumulative reward = Sum[gamma^k*reward_k + k + 1]

    > Types of tasks: Episodic / Continuous
        <> Episode : List of States, Actions, Rewards and New states. Has start and end.
        <> Continuous: Agent has to choose best action by interacting with the environment, continuously

    > Exploring / Exploiting tradeoff:
        <> Exploiting is using up the near resources 
        <> Exploring is learning about the environment by doing random things, and 
        getting bigger rewards

    > How to solve RL? Solution is a Optimal Policy
        <> By using policy: Policy is the brain / function that needs to be learnt to maximize the expected return, 
        when agent does according to it.
        🌱 policy based: Teaching agent to take the next action 
            💢 policy is a function, that keeps mapping of state and the best possible action that one can take. 
                🌀 Deterministic policy : given a state, return same action
                🌀 Stochastic policy : given a state , returns value from prob distribution of actions
        🌱 value based : showing agent which state is more valuable
            💢 value function maps the state with the value of being at that state
            💢 policy in this case will be to go to a state with higher value
             Policy will select the state with maximum returns
    https://huggingface.co/learn/deep-rl-course/unit1/deep-rl
14/02
    > How deep learning is involved?
        <> Will use Deep Neural Networks to find the optimal policy
    > Handson: Lunar Lander on JupyterNotebook (projects\LunarLander\unit1.ipynb)
        <> Learnt that huggingface_sb3 library for loading and uploading models
        <> Libraries involved in creating videos from the captured frames are thought.
        <> leart about os.kill(os.getpid(), 9) to bring down the runtime
        🤔 The notebook was 1st executed directly on colab, so not typing out the 
        commands.(The notebook was introductory, there will be many occasions to train later) 
        <> Will be using the gymnasium maintained by (https://farama.org/projects), where 
        many other like PettingZoo, Minari, Comet, CleanRL and a lot more
        <> Bonus update is on the Unity front, there is https://github.com/Unity-Technologies/ml-agents
        and 
        <> Once the LunarLander environment is understood, then the model needs to be 
        created, using StableBaselines3
        <> PPO is a combination of:
        Value-based reinforcement learning method: learning an action-value function that will tell us the 
        most valuable action to take given a state and action.
        Policy-based reinforcement learning method: learning a policy that will give us a 
        probability distribution over actions.
    🙌 Completed unit1 and bonus unit, will be entering into 
    https://huggingface.co/learn/deep-rl-course/unit2/introduction
    > Setup the environment for RLWork in Kali:
        Bash level:
            - !apt install swig cmake
            - !sudo apt-get update
            - !sudo apt-get install -y python3-opengl
            - !apt install ffmpeg
            - !apt install xvfb
        Pip Level:
            - pyvirtualdisplay(https://github.com/ponty/pyvirtualdisplay/tree/3.0)
            - stable-baselines3==2.0.0a5
            - swig
            - gymnasium[all] (think if entire gym is better to install)
            - huggingface_sb3

    > DeepQLearning : Value Based methods
        - value function maps state to the expected value of being at the state. 
        - policy is not trained, we need to train the value function(neural net). this value function 
        outputs the value of the state or state-action pair. 
        - depending on the value, the policy will take action. 
        - **we have to design a policy** for the value to take the action
        so finding the optimal value function leads to optimal policy. 
        - Epsilon-Greedy policy is used
        - Two Value functions: 
            > State - Value function : Returns expected returns if agent starts at that state, and 
            follows the policy for all future timesteps.
            > Action - Value Function: for each state / action pair the action-value func returns the 
            expected return in that state, if agents starts at that state, takes that action and 
            follows policy for all future timesteps
        - Bellman equation simplifies value estimation for both state-value or action-value function 
         the sum of immediate reward + the discounted value of the state that follows.
         Bellman Eqn: Rt+1 + gamma * V(St+1)
         This is similar to dynamic programming, recursive solution
         - Learning Stategies : Monte-Carlo vs Temporal difference learning. 
         MC uses entire episode of experience before learning, while temporal difference uses 
         only the time-steps to learn.
            (*) With Monte Carlo, we update the value function from a complete episode,
            and so we use the actual accurate discounted return of this episode.

            (*) With TD Learning, we update the value function from a step, and we replace Gt,
            which we don’t know, with an estimated return called the TD target.
        - Q-Learning is the algorithm we use to train our Q-function, an action-value function that 
        determines the value of being at a particular state and taking a specific action at that state
        The Q comes from “the Quality” (the value) of that action at that state.
        - a Q-table that has the value of each state-action pair. Given a state and action, 
        our Q-function will search inside its Q-table to output the value.
        - When the training is done, we have an optimal Q-function, which means we have optimal Q-table.
        💡 Epsilon - Greedy Idea:
        The idea is that, with an initial value of ɛ = 1.0:
            With probability 1 — ɛ : we do exploitation (aka our agent selects the action with the highest state-action pair value).
            With probability ɛ: we do exploration (trying random action).
        the probability of doing exploration will be huge since ɛ is very high, 
        so most of the time, we’ll explore. But as the training goes on, and consequently our 
        Q-table gets better and better in its estimations, we progressively reduce the epsilon value
        💡 off-policy: using a different policy for acting (inference) and updating (training).
        For instance, with Q-Learning, the epsilon-greedy policy (acting policy), is different from the 
        greedy policy that is used to select the best next-state action value to update our Q-value (updating policy).
        💡 on-policy: using a same policy for acting (inference) and updating (training).
15/02
    - HandsON Q-Learning tutorial:
        > Got all the libraries installed, and was able to execute the code successfully. 
        > Use pickle instead of pickle5, it will work
        > Do pip install ipywidgets for the IProgress to work.
        Rest of the entire tutorial works with ease
        > Frozen-Lake and Taxi-v3 maps were loaded from gymnasium.
            - The env is having action_space, action_space.sample(),
            observation_space, observation_space.sample()
            - Q_table is initialized using the state_space and action_space
            with simple array of zeros
            - Greedy Policy is created using np.argmax() on each state_space in the 
            Q_table. On top of the greedy_epsilon_policy is wrapped.
            - Train function contains the env.step(action) which takes the action that is 
            generated by the epsilon_greedy_policy, and gets the state.
            This is updated onto the Q_table.
            - Evaluate function is differing in not calling the greedy_epsilon_policy wrapper
            Rest is followed by evaluation process implemented.
        > Saw that QTable training is completely done by hand by writing the functions, for 
            updating the tables, and then using it to train the model.
        > In addition there is complete process of loading this model and making video using 
        pyvirtual display. (Need to review)
    - Decided to setup the RLEnv inside Windows also.(After the below challenges, installation went through) 
        > pip install pyvirtualdisplay stable-baselines3 gymnasium[all] huggingface_sb3
            Getting Gymnasium to work with Windows is taking considerable downloads.
            🚪 Hit the road-block with swig:
            Swig is the abbreviation of Simplified Wrapper and Interface Generator, it can
            give script language such as python the ability to invoke C and C++ 
            libraries interface method indirectly. 
            💡 Had to install the binary from https://sourceforge.net/projects/swig/
            and extracting that in c:\ and including the folder into the path.
            🚪 Microsoft Visual C++ 14.0 or greater is required.
            > https://wiki.python.org/moin/WindowsCompilers#Microsoft_Visual_C.2B-.2B-_14.2_standalone:_Build_Tools_for_Visual_Studio_2019_.28x86.2C_x64.2C_ARM.2C_ARM64.29
            The above wiki was providing alternate way to get the build tools.
            > Another suggestion is to download the vs_redist from here 
            (https://learn.microsoft.com/en-US/cpp/windows/latest-supported-vc-redist?view=msvc-170#visual-studio-2015-2017-2019-and-2022) 
            > The link that leads to below steps: https://visualstudio.microsoft.com/vs/older-downloads/
                - Started Downloading the Build Tools for Visual Studio 2019 from https://my.visualstudio.com/Downloads
                - The file name is: vs_buildtools__b1860f224b7741939330562fd34bff98.exe. Seems like a older version of build_tools, which only does the build tool 
            installation. The size of download was 1.17GB, install is expected to be twice the size.
    - Unit2: Deep Q-learning with Atari Games:
        ❓ Why Deep? When the state space is gigantic, creating and updating a Q-table for that environment would not be efficient. 
        In this case, the best idea is to approximate the Q-values using a parametrized Q-function 
        💡 Given a state, the different Q-values for each possible action at that state. And that’s exactly what Deep Q-Learning does.
        💡 We take a stack of 4 frames passed through the network as a state and output a vector of Q-values for each
        possible action at that state. Then, like with Q-Learning, we just need to use our epsilon-greedy policy to select which action to take.
        💡 The frames are stacked to overcome the temporal limitations, like knowing the direction of the object movement in frames 
        🆔 Deep Q-Learning, we create a loss function that compares our Q-value prediction and
        the Q-target and uses gradient descent to update the weights of our Deep Q-Network to approximate our Q-values better. 
16/02
    - Windows Install: (not working due to challenge with the VCRedist)
        > Gymnasium import threw FileNotFoundError: Could not find module 'msvcp140.dll'
        The issue seems to be VC++ pack is not installed. Trying to re-install the pack by 
        download the VCredist still did not work 
    TD : Temporal Difference 
    - continuing with https://huggingface.co/learn/deep-rl-course/unit3/deep-q-algorithm
    🌛 The Deep Q-Learning training algorithm has two phases:
        - Sampling: we perform actions and store the observed experience tuples in a replay memory.
        - Training: Select a small batch of tuples randomly and learn from this batch using a gradient descent update step.    
        To help us stabilize the training, we implement three different solutions:
            -> Experience Replay to make more efficient use of experiences.
            -> Fixed Q-Target to stabilize the training.
            -> Double Deep Q-Learning, to handle the problem of the overestimation of Q-values.
        - Experiences Replay:
            -> gets experiences (state, action, reward, and next state) & updates NN 
            -> reusing the experiences and sample them during training
            -> avoid catastrophic forgetting, reduce correlation
            -> avoid oscillating / diverging action values
        - Fixed Q-Target:
            -> When we want to calculate the TD error (aka the loss), we calculate the
            difference between the TD target (Q-Target) and the current Q-value (estimation of Q).
            ->  don’t have any idea of the real TD target. We need to estimate it. Using the Bellman 
            equation, we saw that the TD target is just the reward of taking that action at that state 
            plus the discounted highest Q value for the next state.
            -> Use a separate network with fixed parameters for estimating the TD Target
            -> Copy the parameters from our Deep Q-Network every C steps to update the target network.
        - Double deep-q learning:
            -> https://papers.nips.cc/paper_files/paper/2010/hash/091d584fced301b442654dd8c23b3fc9-Abstract.html
        - handson uses CNN policy and trains on AtariSpace Invaders. There will 10M training steps, which 
        requires atleast 5 hours. So need to run train on local GPU.
        - Need to install rl_baselines_zoo and rl_agents (https://rl-baselines3-zoo.readthedocs.io/)
        - https://youtube.com/playlist?list=PLwRJQ4m4UJjNymuBM9RdmB3Z9N5-0IlY0&si=mIPln3u6TAd7Ug5s
        The above playlist provides the basic of RL.(Foundations of Deep RL -- 6-lecture series) 
        - https://www.youtube.com/watch?v=sPEEV8Xih20&list=PLRqwX-V7Uu6YJ3XfHhT2Mm4Y5I99nrIKX
        Shiffman has a interesting series on algorithm and learning. 
    - Bonus unit-3: Automatic HyperParameter Tuning with Optuna
        - Theory of searching HyperParameter Automatically:
            https://youtu.be/AidFTOdGNFQ
        - Optuna HandsON:
            https://youtu.be/ihP7E76KGOI
    - Unit 4: Policy Gradient with PyTorch
        Some things new:
        https://spinningup.openai.com/  
         Objective: Implement policy gradient algorithm called Monte Carlo Reinforce from 
         scratch using PyTorch
         Steps of Policy Based RL:
            -> parameterize the policy, using a neural net pi-0, where policy will output probability 
            distribution over actions.
            -> objective then is to maximize the performance of the parameterized policy using gradient ascent.
            -> control parameter is theta to affect the distribution of actions over state.
            There are two sub-classes in policy-based methods:
            The difference between these two methods lies on how we optimize the parameter
                * In policy-based methods, we search directly for the optimal policy. We can optimize the parameter θ 
                indirectly by maximizing the local approximation of the objective function with techniques like hill 
                climbing, simulated annealing, or evolution strategies.
                * In policy-gradient methods, because it is a subclass of the policy-based methods, we search directly 
                for the optimal policy. But we optimize the parameter θ directly by performing the gradient ascent on 
                the performance of the objective function J(θ).
                Benefits:
                    Policy-gradient methods can learn a stochastic policy while value functions can’t.
                    This has two consequences:
                    -> We don’t need to implement an exploration/exploitation trade-off by hand. 
                    Since we output a probability distribution over actions, the agent explores the state 
                    space without always taking the same trajectory.
                    -> We also get rid of the problem of perceptual aliasing. Perceptual aliasing is 
                    when two states seem (or are) the same but need different actions. 
                    -> Policy-gradient methods are more effective in high-dimensional action 
                    spaces and continuous actions spaces
                    -> policy-gradient methods, we output a probability distribution over actions.
                    -> Policy-gradient methods have better convergence properties
                Cons:
                    - Frequently, policy-gradient methods converges to a local maximum instead of a 
                    global optimum.
                    - Policy-gradient goes slower, step by step: it can take longer to train (inefficient).
                    - Policy-gradient can have high variance. We’ll see in the actor-critic unit why,
                    and how we can solve this problem.
            Diving into Policy Gradient Method:
            -> The idea is that we have a parameterized stochastic policy. In our case, a neural network outputs
            a probability distribution over actions. The probability of taking each action is also called the 
            action preference.
            -> Policy-gradient is to control the probability distribution of actions by tuning the policy 
            such that good actions (that maximize the return) are sampled more frequently in the future
            -> The idea is that we’re going to let the agent interact during an episode. And if we win the 
            episode, we consider that each action taken was good and must be more sampled in the future since 
            they lead to win.So for each state-action pair, we want to increase the P(a∣s): the probability of 
            taking that action at that state. Or decrease if we lost.
            -> The objective function gives us the performance of the agent given a trajectory (state action sequence 
            without considering reward (contrary to an episode)), and it outputs the expected cumulative reward.
            ->  gradient-ascent. It’s the inverse of gradient-descent since it gives the direction of the
            steepest increase of J(θ).
            2 Problems:
            -> Calculating true gradient can be super expensive, since calculating each trajectory will be computationally 
            expensive.  So we want to calculate a gradient estimation with a sample-based estimate (collect some trajectories).
            -> o differentiate this objective function, we need to differentiate the state distribution, called the Markov Decision 
            Process dynamics. This is attached to the environment. It gives us the probability of the environment going into 
            the next state, given the current state and the action taken by the agent. 
            -> we’re going to use a solution called the Policy Gradient Theorem that will help us to reformulate the 
            objective function into a differentiable function that does not involve the differentiation of the state distribution.
            -> The Reinforce algorithm, also called Monte-Carlo policy-gradient, is a policy-gradient algorithm that uses an 
            estimated return from an entire episode to update the policy parameter θ:
    - unit-5: Intro to ML Agents
        - Unity ML-Agents is a toolkit for the game engine Unity that allows us to create environments using Unity or use 
        pre-made environments to train our agents.
        - 6 components:
            -> The first is the Learning Environment, which contains the Unity scene (the environment) and the environment elements (game characters).
                - The first is the agent component, the actor of the scene. We’ll train the agent by optimizing its policy (which will tell us what action to take in each state).
                    The policy is called the Brain.
                - Finally, there is the Academy. This component orchestrates agents and their decision-making processes. Think of this Academy as a teacher who handles Python API requests    
                The Academy will be the one that will send the order to our Agents and ensure that agents are in sync:
                    - Collect Observations
                    - Select your action using your policy
                    - Take the Action
                    - Reset if you reached the max step or if you’re done.
            -> The second is the Python Low-level API, which contains the low-level Python interface for interacting and manipulating the environment. It’s the API we use to launch the training.
            -> Then, we have the External Communicator that connects the Learning Environment (made with C#) with the low level Python API (Python).
            -> The Python trainers: the Reinforcement algorithms made with PyTorch (PPO, SAC…).
            -> The Gym wrapper: to encapsulate the RL environment in a gym wrapper.
            -> The PettingZoo wrapper: PettingZoo is the multi-agents version of the gym wrapper.
        - snowball and pyramid environment is explained in detail:
            -> you need to avoid the reward engineering problem, which is having a too complex reward function to force your agent to behave as you want it to do. Why? Because by doing that,
            you might miss interesting strategies that the agent will find with a simpler reward function.
            Why Curiosity?:
            -> introducing the curiosity driven next state predictions. (https://medium.com/data-from-the-trenches/curiosity-driven-learning-through-next-state-prediction-f7f4e2f592fa) 
            -> Random Network Distillation (https://medium.com/data-from-the-trenches/curiosity-driven-learning-through-random-network-distillation-488ffd8e5938)
            -> 2 problems with RL:
                - First, the sparse rewards problem: that is, most rewards do not contain information, and hence are set to zero.
                - second big problem is that the extrinsic reward function is handmade; in each environment, a human has to implement 
                a reward function. But how we can scale that in big and complex environments?
            -> So Curiosity is : This intrinsic reward mechanism is known as Curiosity because this reward pushes the agent to explore states that are novel/unfamiliar.
            To achieve that, our agent will receive a high reward when exploring new trajectories. 
            -> Calculate Curiosity as the error of our agent in predicting the 
            next state, given the current state and action taken.
            -> Curiosity is to encourage our agent to perform actions that reduce the uncertainty in the agent’s ability to predict the consequences of its actions (uncertainty 
            will be higher in areas where the agent has spent less time or in areas with complex dynamics). 
            -> ML-Agents uses a more advanced one called Curiosity through random network distillation. This is out of the scope of the tutorial but if you’re interested 
    - Unit-6: Actor Critic method
        - Challenge of Variance in Reinforce.
            -> In monte-carlo sampling, the trajectory is collected and discounted returns are calculated. If the returns are good, then all the actions are reinforced. Method 
            has advantage of being un-biased.
            -> Due to stochastic(random events in an episode) and stochasticity in policy the trajectories can lead to different returns, 
            leading to high variance
            -> High bias : Underfit / Just Right / High Variance: Over fit
            https://blog.mlreview.com/making-sense-of-the-bias-variance-trade-off-in-deep-reinforcement-learning-79cf1e83d565

        - Idea behind Actor Critic method:
            > This is the idea behind Actor-Critic. We learn two function approximations:
                - A policy function that controls how our agent acts. 
                    * Takes the current state from the environment and pass it thru policy 
                    * Policy outputs an Action
                - A value function to assist the policy update by measuring how good the action taken is.
                    * Value function takes state and action, and computes value and return Q-Value     
            > Advantage function compares the relative advantage of action compared to the other 
            possible at a state. How taking that action at a state is better compared to the average 
            value of the state. 
            > Fortunately, we can use the TD error as a good estimator of the advantage function.
        - Dense Vs Sparse reward:
            * We’re going to use the dense version of this environment. It means we’ll get a dense reward function that will provide a reward at each 
            timestep (the closer the agent is to completing the task, the higher the reward). Contrary to a sparse reward function where 
            the environment return a reward if and only if the task is completed.
        - The model training is a straightforward using the A2C class that is part of the stablebaselines
    - Unit-7: Introducing Multi-agent Environment (Multi-Agent Reinforce Learning)
        - we learned to train agents in a single-agent system where our agent was alone in its environment: it was not cooperating or 
        collaborating with other agents.
        - Two types of Environment: Cooperative Vs Competitive or Adverserial Vs Mixed 
        - How to Train multi-agents:
            * Decentralized training: Agents are designed and trained individually without sharing info 
            with other agents. The tech will make the environment non-stationary. The other agents will 
            move, and considered as part of the environment.
            * Centralized Training: High level process collect the agents experiences using exp-buffers 
            and use it learn common-policy
        - Self-Play :  In self-play, the agent uses former copies of itself (of its policy) as an opponent. This way, the agent 
        will play against an agent of the same level (challenging but not too much), have opportunities to gradually improve its 
        policy, and then update its opponent as it becomes better. It’s a way to bootstrap an opponent and 
        progressively increase the opponent’s complexity.
        https://blog.unity.com/engine-platform/training-intelligent-adversaries-using-self-play-with-ml-agents
        - Some parameters to control:
            * How often we change opponents with the swap_steps and team_change parameters.
            * The number of opponents saved with the window parameter. A larger value of window  means that an agent’s pool of opponents will contain a larger diversity of behaviors since it will contain policies from earlier in the training run.
            * The probability of playing against the current self vs opponent sampled from the pool with play_against_latest_model_ratio. A larger value of play_against_latest_model_ratio  indicates that an agent will be playing against the current opponent more often.
            * The number of training steps before saving a new opponent with save_steps parameters.
        - n ELO rating system (named after Arpad Elo) that calculates the relative skill level between 2 players from a given population in a zero-sum game.
        - The winning player takes points from the losing one.
            * The number of points is determined by the difference in the 2 players ratings (hence relative).
            * If the higher-rated player wins → few points will be taken from the lower-rated player.
            * If the lower-rated player wins → a lot of points will be taken from the high-rated player.
            * If it’s a draw → the lower-rated player gains a few points from the higher.
        - When multi-agents are involved, and they have to cooperate as a team, and compete with another team, 
        there needs to be a different algo. he Unity MLAgents team developed the solution in a new multi-agent trainer 
        called MA-POCA (Multi-Agent POsthumous Credit Assignment)
        - The idea is simple but powerful: a centralized critic processes the states of all agents in the 
        team to estimate how well each agent is doing. Think of this critic as a coach.
    - Unit-8: Proximal Policy Optimisation:
        - Proximal Policy Optimization (PPO), an architecture that improves our agent’s training stability by 
        avoiding policy updates that are too large. To do that, we use a ratio that indicates the 
        difference between our current and old policy and clip this ratio to a specific range
        [1−ϵ,1+ϵ] . 
        For two reasons:
            * We know empirically that smaller policy updates during training are more likely to converge 
            to an optimal solution.
            * A too-big step in a policy update can result in falling “off the cliff” (getting a 
            bad policy) and taking a long time or even having no possibility to recover.
        - The idea is to constrain our policy update with a new objective function called the 
        Clipped surrogate objective function that will constrain the policy change in a small range 
        using a clip.
        By clipping the ratio, we ensure that we do not have a too large policy update because the current policy can’t be too different from the older one.
        To do that, we have two solutions:
            - TRPO (Trust Region Policy Optimization) uses KL divergence constraints outside the objective function to constrain the policy update. But this method is complicated to implement and takes more computation time.
            - PPO clip probability ratio directly in the objective function with its Clipped surrogate objective function.
    - Additional: Model Based RL 
        - There is an agent that repeatedly tries to solve a problem, accumulating state and action data.
        - With that data, the agent creates a structured learning tool, a dynamics model, to reason about the world.
        - With the dynamics model, the agent decides how to act by predicting the future.
        - With those actions, the agent collects more data, improves said model, and hopefully improves future actions.
    - Offline RL: https://www.youtube.com/watch?v=k08N5a0gG0A 
    - RLHF: Reinforcement learning from human feedback (RLHF) is a methodology for integrating human data labels into a RL-based optimization 
    process. It is motivated by the challenge of modeling human preferences.
        - Read 1: https://huggingface.co/blog/rlhf
        - watch 2: https://youtu.be/2MBJOuVq380
    - Intro to Decision Transformers : https://huggingface.co/blog/decision-transformers
