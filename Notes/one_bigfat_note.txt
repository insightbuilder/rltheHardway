13/02
- Diving into RL Basics:
    > RL Official definition: Solving control / decision making tasks by building agents that interact and learnt from environment by getting rewards
    > RL FrameWork: State, Action, Reward, Next State. (even not being dead is a reward!!!)
    > RL goal is to maximize the "Expected Return", or in other words Cumulative Reward
    > Markov Property: States that Agent needs only the current state to take act/ take decision, not the entire history of states
    > Observed State is partial description of the environment(2dScroller), State is "complete" description of the environment(game board). 
    > Action Space: Set of possible action that can be taken by Agent. Continuous(selfDriving Car) or Discrete(Mario game, 4 directions)
    > Reward and discounting is the only feedback to the agent in RL. More predictable near term rewards, Vs unpredictable longterm reward. 
        <> Discounting happens depending on whether we are likely to get the reward, (as it could be dangerous or unavailable)
        <> Or the reward will be available in the future
        ðŸŽ‡ Measured using Gamma: Between 0 and 1 usually 0.95 to 0.99.  
        ðŸ¥Œ leads to discounted expected cumulative reward = Sum[gamma^k*reward_k + k + 1]

    > Types of tasks: Episodic / Continuous
        <> Episode : List of States, Actions, Rewards and New states. Has start and end.
        <> Continuous: Agent has to choose best action by interacting with the environment, continuously

    > Exploring / Exploiting tradeoff:
        <> Exploiting is using up the near resources 
        <> Exploring is learning about the environment by doing random things, and 
        getting bigger rewards

    > How to solve RL? Solution is a Optimal Policy
        <> By using policy: Policy is the brain / function that needs to be learnt to maximize the expected return, 
        when agent does according to it.
        ðŸŒ± policy based: Teaching agent to take the next action 
            ðŸ’¢ policy is a function, that keeps mapping of state and the best possible action that one can take. 
                ðŸŒ€ Deterministic policy : given a state, return same action
                ðŸŒ€ Stochastic policy : given a state , returns value from prob distribution of actions
        ðŸŒ± value based : showing agent which state is more valuable
            ðŸ’¢ value function maps the state with the value of being at that state
            ðŸ’¢ policy in this case will be to go to a state with higher value
             Policy will select the state with maximum returns
    https://huggingface.co/learn/deep-rl-course/unit1/deep-rl
14/02
    > How deep learning is involved?
        <> Will use Deep Neural Networks to find the optimal policy
    > Handson: Lunar Lander on JupyterNotebook (projects\LunarLander\unit1.ipynb)
        <> Learnt that huggingface_sb3 library for loading and uploading models
        <> Libraries involved in creating videos from the captured frames are thought.
        <> leart about os.kill(os.getpid(), 9) to bring down the runtime
        ðŸ¤” The notebook was 1st executed directly on colab, so not typing out the 
        commands.(The notebook was introductory, there will be many occasions to train later) 
        <> Will be using the gymnasium maintained by (https://farama.org/projects), where 
        many other like PettingZoo, Minari, Comet, CleanRL and a lot more
        <> Bonus update is on the Unity front, there is https://github.com/Unity-Technologies/ml-agents
        and 
        <> Once the LunarLander environment is understood, then the model needs to be 
        created, using StableBaselines3
        <> PPO is a combination of:
        Value-based reinforcement learning method: learning an action-value function that will tell us the 
        most valuable action to take given a state and action.
        Policy-based reinforcement learning method: learning a policy that will give us a 
        probability distribution over actions.
    ðŸ™Œ Completed unit1 and bonus unit, will be entering into 
    https://huggingface.co/learn/deep-rl-course/unit2/introduction
    > Setup the environment for RLWork in Kali:
        Bash level:
            - !apt install swig cmake
            - !sudo apt-get update
            - !sudo apt-get install -y python3-opengl
            - !apt install ffmpeg
            - !apt install xvfb
        Pip Level:
            - pyvirtualdisplay(https://github.com/ponty/pyvirtualdisplay/tree/3.0)
            - stable-baselines3==2.0.0a5
            - swig
            - gymnasium[all] (think if entire gym is better to install)
            - huggingface_sb3

    > DeepQLearning : Value Based methods
        - value function maps state to the expected value of being at the state. 
        - policy is not trained, we need to train the value function(neural net). this value function 
        outputs the value of the state or state-action pair. 
        - depending on the value, the policy will take action. 
        - **we have to design a policy** for the value to take the action
        so finding the optimal value function leads to optimal policy. 
        - Epsilon-Greedy policy is used
        - Two Value functions: 
            > State - Value function : Returns expected returns if agent starts at that state, and 
            follows the policy for all future timesteps.
            > Action - Value Function: for each state / action pair the action-value func returns the 
            expected return in that state, if agents starts at that state, takes that action and 
            follows policy for all future timesteps
        - Bellman equation simplifies value estimation for both state-value or action-value function 
         the sum of immediate reward + the discounted value of the state that follows.
         Bellman Eqn: Rt+1 + gamma * V(St+1)
         This is similar to dynamic programming, recursive solution
         - Learning Stategies : Monte-Carlo vs Temporal difference learning. 
         MC uses entire episode of experience before learning, while temporal difference uses 
         only the time-steps to learn.
            (*) With Monte Carlo, we update the value function from a complete episode,
            and so we use the actual accurate discounted return of this episode.

            (*) With TD Learning, we update the value function from a step, and we replace Gt,
            which we donâ€™t know, with an estimated return called the TD target.
        - Q-Learning is the algorithm we use to train our Q-function, an action-value function that 
        determines the value of being at a particular state and taking a specific action at that state
        The Q comes from â€œthe Qualityâ€ (the value) of that action at that state.
        - a Q-table that has the value of each state-action pair. Given a state and action, 
        our Q-function will search inside its Q-table to output the value.
        - When the training is done, we have an optimal Q-function, which means we have optimal Q-table.
        ðŸ’¡ Epsilon - Greedy Idea:
        The idea is that, with an initial value of É› = 1.0:
            With probability 1 â€” É› : we do exploitation (aka our agent selects the action with the highest state-action pair value).
            With probability É›: we do exploration (trying random action).
        the probability of doing exploration will be huge since É› is very high, 
        so most of the time, weâ€™ll explore. But as the training goes on, and consequently our 
        Q-table gets better and better in its estimations, we progressively reduce the epsilon value
        ðŸ’¡ off-policy: using a different policy for acting (inference) and updating (training).
        For instance, with Q-Learning, the epsilon-greedy policy (acting policy), is different from the 
        greedy policy that is used to select the best next-state action value to update our Q-value (updating policy).
        ðŸ’¡ on-policy: using a same policy for acting (inference) and updating (training).
    15/02
        - HandsON Q-Learning tutorial:
            > Got all the libraries installed, and was able to execute the code successfully. 
            > Use pickle instead of pickle5, it will work
            > Do pip install ipywidgets for the IProgress to work.
            Rest of the entire tutorial works with ease
            > Frozen-Lake and Taxi-v3 maps were loaded from gymnasium.
                - The env is having action_space, action_space.sample(),
                observation_space, observation_space.sample()
                - Q_table is initialized using the state_space and action_space
                with simple array of zeros
                - Greedy Policy is created using np.argmax() on each state_space in the 
                Q_table. On top of the greedy_epsilon_policy is wrapped.
                - Train function contains the env.step(action) which takes the action that is 
                generated by the epsilon_greedy_policy, and gets the state.
                This is updated onto the Q_table.
                - Evaluate function is differing in not calling the greedy_epsilon_policy wrapper
                Rest is followed by evaluation process implemented.
            > Saw that QTable training is completely done by hand by writing the functions, for 
             updating the tables, and then using it to train the model.
            > In addition there is complete process of loading this model and making video using 
            pyvirtual display. (Need to review)
