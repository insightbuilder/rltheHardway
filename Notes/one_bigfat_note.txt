13/02
- Diving into RL Basics:
    > RL Official definition: Solving control / decision making tasks by building agents that interact and learnt from environment by getting rewards
    > RL FrameWork: State, Action, Reward, Next State. (even not being dead is a reward!!!)
    > RL goal is to maximize the "Expected Return", or in other words Cumulative Reward
    > Markov Property: States that Agent needs only the current state to take act/ take decision, not the entire history of states
    > Observed State is partial description of the environment(2dScroller), State is "complete" description of the environment(game board). 
    > Action Space: Set of possible action that can be taken by Agent. Continuous(selfDriving Car) or Discrete(Mario game, 4 directions)
    > Reward and discounting is the only feedback to the agent in RL. More predictable near term rewards, Vs unpredictable longterm reward. 
        <> Discounting happens depending on whether we are likely to get the reward, (as it could be dangerous or unavailable)
        <> Or the reward will be available in the future
        üéá Measured using Gamma: Between 0 and 1 usually 0.95 to 0.99.  
        ü•å leads to discounted expected cumulative reward = Sum[gamma^k*reward_k + k + 1]

    > Types of tasks: Episodic / Continuous
        <> Episode : List of States, Actions, Rewards and New states. Has start and end.
        <> Continuous: Agent has to choose best action by interacting with the environment, continuously

    > Exploring / Exploiting tradeoff:
        <> Exploiting is using up the near resources 
        <> Exploring is learning about the environment by doing random things, and 
        getting bigger rewards

    > How to solve RL? Solution is a Optimal Policy
        <> By using policy: Policy is the brain / function that needs to be learnt to maximize the expected return, 
        when agent does according to it.
        üå± policy based: Teaching agent to take the next action 
            üí¢ policy is a function, that keeps mapping of state and the best possible action that one can take. 
                üåÄ Deterministic policy : given a state, return same action
                üåÄ Stochastic policy : given a state , returns value from prob distribution of actions
        üå± value based : showing agent which state is more valuable
            üí¢ value function maps the state with the value of being at that state
            üí¢ policy in this case will be to go to a state with higher value
             Policy will select the state with maximum returns
    https://huggingface.co/learn/deep-rl-course/unit1/deep-rl
14/02
    > How deep learning is involved?
        <> Will use Deep Neural Networks to find the optimal policy
    > Handson: Lunar Lander on JupyterNotebook (projects\LunarLander\unit1.ipynb)
        <> Learnt that huggingface_sb3 library for loading and uploading models
        <> Libraries involved in creating videos from the captured frames are thought.
        <> leart about os.kill(os.getpid(), 9) to bring down the runtime
        ü§î The notebook was 1st executed directly on colab, so not typing out the 
        commands.(The notebook was introductory, there will be many occasions to train later) 
        <> Will be using the gymnasium maintained by (https://farama.org/projects), where 
        many other like PettingZoo, Minari, Comet, CleanRL and a lot more
        <> Bonus update is on the Unity front, there is https://github.com/Unity-Technologies/ml-agents
        and 
        <> Once the LunarLander environment is understood, then the model needs to be 
        created, using StableBaselines3
        <> PPO is a combination of:
        Value-based reinforcement learning method: learning an action-value function that will tell us the 
        most valuable action to take given a state and action.
        Policy-based reinforcement learning method: learning a policy that will give us a 
        probability distribution over actions.
    üôå Completed unit1 and bonus unit, will be entering into 
    https://huggingface.co/learn/deep-rl-course/unit2/introduction
    > Setup the environment for RLWork in Kali:
        Bash level:
            - !apt install swig cmake
            - !sudo apt-get update
            - !sudo apt-get install -y python3-opengl
            - !apt install ffmpeg
            - !apt install xvfb
        Pip Level:
            - pyvirtualdisplay(https://github.com/ponty/pyvirtualdisplay/tree/3.0)
            - stable-baselines3==2.0.0a5
            - swig
            - gymnasium[all] (think if entire gym is better to install)
            - huggingface_sb3

    > DeepQLearning : Value Based methods
        - value function maps state to the expected value of being at the state. 
        - policy is not trained, we need to train the value function(neural net). this value function 
        outputs the value of the state or state-action pair. 
        - depending on the value, the policy will take action. 
        - **we have to design a policy** for the value to take the action
        so finding the optimal value function leads to optimal policy. 
        - Epsilon-Greedy policy is used
        - Two Value functions: 
            > State - Value function : Returns expected returns if agent starts at that state, and 
            follows the policy for all future timesteps.
            > Action - Value Function: for each state / action pair the action-value func returns the 
            expected return in that state, if agents starts at that state, takes that action and 
            follows policy for all future timesteps
        - Bellman equation simplifies value estimation for both state-value or action-value function 
         the sum of immediate reward + the discounted value of the state that follows.
         Bellman Eqn: Rt+1 + gamma * V(St+1)
         This is similar to dynamic programming, recursive solution
         - Learning Stategies : Monte-Carlo vs Temporal difference learning. 
         MC uses entire episode of experience before learning, while temporal difference uses 
         only the time-steps to learn.
            (*) With Monte Carlo, we update the value function from a complete episode,
            and so we use the actual accurate discounted return of this episode.

            (*) With TD Learning, we update the value function from a step, and we replace Gt,
            which we don‚Äôt know, with an estimated return called the TD target.
        - Q-Learning is the algorithm we use to train our Q-function, an action-value function that 
        determines the value of being at a particular state and taking a specific action at that state
        The Q comes from ‚Äúthe Quality‚Äù (the value) of that action at that state.
        - a Q-table that has the value of each state-action pair. Given a state and action, 
        our Q-function will search inside its Q-table to output the value.
        - When the training is done, we have an optimal Q-function, which means we have optimal Q-table.
        üí° Epsilon - Greedy Idea:
        The idea is that, with an initial value of …õ = 1.0:
            With probability 1 ‚Äî …õ : we do exploitation (aka our agent selects the action with the highest state-action pair value).
            With probability …õ: we do exploration (trying random action).
        the probability of doing exploration will be huge since …õ is very high, 
        so most of the time, we‚Äôll explore. But as the training goes on, and consequently our 
        Q-table gets better and better in its estimations, we progressively reduce the epsilon value
        üí° off-policy: using a different policy for acting (inference) and updating (training).
        For instance, with Q-Learning, the epsilon-greedy policy (acting policy), is different from the 
        greedy policy that is used to select the best next-state action value to update our Q-value (updating policy).
        üí° on-policy: using a same policy for acting (inference) and updating (training).
15/02
    - HandsON Q-Learning tutorial:
        > Got all the libraries installed, and was able to execute the code successfully. 
        > Use pickle instead of pickle5, it will work
        > Do pip install ipywidgets for the IProgress to work.
        Rest of the entire tutorial works with ease
        > Frozen-Lake and Taxi-v3 maps were loaded from gymnasium.
            - The env is having action_space, action_space.sample(),
            observation_space, observation_space.sample()
            - Q_table is initialized using the state_space and action_space
            with simple array of zeros
            - Greedy Policy is created using np.argmax() on each state_space in the 
            Q_table. On top of the greedy_epsilon_policy is wrapped.
            - Train function contains the env.step(action) which takes the action that is 
            generated by the epsilon_greedy_policy, and gets the state.
            This is updated onto the Q_table.
            - Evaluate function is differing in not calling the greedy_epsilon_policy wrapper
            Rest is followed by evaluation process implemented.
        > Saw that QTable training is completely done by hand by writing the functions, for 
            updating the tables, and then using it to train the model.
        > In addition there is complete process of loading this model and making video using 
        pyvirtual display. (Need to review)
    - Decided to setup the RLEnv inside Windows also.(After the below challenges, installation went through) 
        > pip install pyvirtualdisplay stable-baselines3 gymnasium[all] huggingface_sb3
            Getting Gymnasium to work with Windows is taking considerable downloads.
            üö™ Hit the road-block with swig:
            Swig is the abbreviation of Simplified Wrapper and Interface Generator, it can
            give script language such as python the ability to invoke C and C++ 
            libraries interface method indirectly. 
            üí° Had to install the binary from https://sourceforge.net/projects/swig/
            and extracting that in c:\ and including the folder into the path.
            üö™ Microsoft Visual C++ 14.0 or greater is required.
            > https://wiki.python.org/moin/WindowsCompilers#Microsoft_Visual_C.2B-.2B-_14.2_standalone:_Build_Tools_for_Visual_Studio_2019_.28x86.2C_x64.2C_ARM.2C_ARM64.29
            The above wiki was providing alternate way to get the build tools.
            > Another suggestion is to download the vs_redist from here 
            (https://learn.microsoft.com/en-US/cpp/windows/latest-supported-vc-redist?view=msvc-170#visual-studio-2015-2017-2019-and-2022) 
            > The link that leads to below steps: https://visualstudio.microsoft.com/vs/older-downloads/
                - Started Downloading the Build Tools for Visual Studio 2019 from https://my.visualstudio.com/Downloads
                - The file name is: vs_buildtools__b1860f224b7741939330562fd34bff98.exe. Seems like a older version of build_tools, which only does the build tool 
            installation. The size of download was 1.17GB, install is expected to be twice the size.
    - Unit2: Deep Q-learning with Atari Games:
        ‚ùì Why Deep? When the state space is gigantic, creating and updating a Q-table for that environment would not be efficient. 
        In this case, the best idea is to approximate the Q-values using a parametrized Q-function 
        üí° Given a state, the different Q-values for each possible action at that state. And that‚Äôs exactly what Deep Q-Learning does.
        üí° We take a stack of 4 frames passed through the network as a state and output a vector of Q-values for each
        possible action at that state. Then, like with Q-Learning, we just need to use our epsilon-greedy policy to select which action to take.
        üí° The frames are stacked to overcome the temporal limitations, like knowing the direction of the object movement in frames 
        üÜî Deep Q-Learning, we create a loss function that compares our Q-value prediction and
        the Q-target and uses gradient descent to update the weights of our Deep Q-Network to approximate our Q-values better. 